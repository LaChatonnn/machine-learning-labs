{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab7_gbt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPeI2IfTuSeYxRqOwUMqUeM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YipingNUS/machine-learning-labs/blob/master/lab7_gbt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCN-3-e4PsSd",
        "colab_type": "text"
      },
      "source": [
        "# Lab 7: Gradient Boosted Trees\n",
        "\n",
        "In this lab, you'll explore gradient boosted trees, one of the most popular method used in Kaggle winning solutions ([reference](https://www.kaggle.com/bigfatdata/what-algorithms-are-most-successful-on-kaggle)). \n",
        "\n",
        "While as usual sklearn implements [gradient boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting), [XGBoost](https://xgboost.readthedocs.io/en/latest/get_started.html) is by far the best and the most optimized library for gradient boosting. Some useful links below. \n",
        "\n",
        "* [The Python programming API](https://xgboost.readthedocs.io/en/latest/python/index.html): focuses only on programming API. The interface is similar to sklearn.\n",
        "* [Introduction to the boosted trees](https://xgboost.readthedocs.io/en/latest/tutorials/model.html): gives an overview how the gradient boosted trees work. You probably don't need to understand it to complete this lab, but it's good to know."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgO2TD78CU-o",
        "colab_type": "text"
      },
      "source": [
        "Different from previous labs, we'll explore a dataset from Kaggle this time, namely the famous HelloWorld competition [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/). Your tasks are as follows:\n",
        "\n",
        "1. Download the dataset (either download as a zip file or use the Kaggle API). For this you'll need to sign up a Kaggle account.\n",
        "2. Take a look at the rules of the competition, especially the evaluation metric.\n",
        "3. Build a GBT model using XGBoost and try to obtain the best performance possible on the test set.\n",
        "4. Compare your result with Kaggle leaderboard and see where you're in the competition. (don't be discouraged if you're placed at the bottom. It's very rare for a newbie to enter top 10% in a Kaggle competion).\n",
        "5. (OPTIONAL): submit an entry to the Kaggle competition. Whora, you just completed your first Kaggle competion. Worth adding to your resume :)\n",
        "\n",
        "Have fun!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmFJmn0MPoYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vWi5OSYw9J1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: your code here."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}